{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle competition Machine learning\n",
    "by \n",
    "Sidney Gharib - 2145565,\n",
    "Elizabteh Michaud - 2073093 and\n",
    "Alexis Nicolas - 2143258"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "from data_processing import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_training_data()\n",
    "X_testing = get_testing_data()\n",
    "#Unnamed: 0 is the Id of the row, we can drop it\n",
    "X = X.drop('Unnamed: 0', axis=1 )\n",
    "\n",
    "x_testing_id = X_testing['Unnamed: 0']\n",
    "X_testing = X_testing.drop('Unnamed: 0', axis=1 )\n",
    "# Unnamed: 0 is the Id of the row, we can drop it\n",
    "y = y.drop(columns=['Unnamed: 0'])  # If the first column is labeled as 'Unnamed: 0'\n",
    "# This variable is only used for the final test, to prevent long grid search from running\n",
    "isFinal = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of elements :', len(X))\n",
    "print('Number of features :', len(X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('len(y):', len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = {}\n",
    "for col in X.columns:\n",
    "    unique_values[col] = X[col].value_counts().shape[0]\n",
    "\n",
    "pd.DataFrame(unique_values, index=['unique value count']).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "bmi_category_order = ['Underweight', 'Normal weight', 'Overweight', 'Obese']\n",
    "age_group_order = ['Young']\n",
    "education_level_order = ['No High School', 'High School Graduate', 'Some College']\n",
    "income_group_order = ['Low Income', 'Middle Income', 'High Income']\n",
    "\n",
    "def ordinal_feature_encoding(x):\n",
    "  non_numerical_col = x.select_dtypes(exclude=['number']).columns\n",
    "  ordinal_encoder = OrdinalEncoder(categories=[bmi_category_order, age_group_order, education_level_order, income_group_order])\n",
    "  x[non_numerical_col] = ordinal_encoder.fit_transform(x[non_numerical_col])\n",
    "\n",
    "  return x\n",
    "\n",
    "X = ordinal_feature_encoding(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,25))\n",
    "for i,col in enumerate(['BMI', 'GenHlth', 'MentHlth', 'PhysHlth', 'Age','Education', 'Income']):\n",
    "    plt.subplot(4,2,i+1)\n",
    "    plt.title('Distribution of '+col)\n",
    "    plt.boxplot(x = col, data = X, vert=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['HighBP', 'HighChol', 'CholCheck', 'Smoker', 'Stroke', 'HeartDiseaseorAttack', \n",
    "        'PhysActivity', 'Veggies', 'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'DiffWalk']\n",
    "\n",
    "def create_plot_pivot(data, x_column):\n",
    "    \"\"\"Create a pivot table for satisfaction versus another rating for easy plotting.\"\"\"\n",
    "    # Merge `X` and `y` to ensure access to 'Diabetes_binary' column for grouping\n",
    "    data_with_target = data.copy()\n",
    "    data_with_target['Diabetes_binary'] = y['Diabetes_binary']\n",
    "    data_with_target['Diabetes_binary'] = data_with_target['Diabetes_binary'].replace({0: 'No Diabetes', 1: 'Diabetes'})\n",
    "    \n",
    "    _df_plot = data_with_target.groupby([x_column, 'Diabetes_binary']).size() \\\n",
    "                   .reset_index().pivot(columns='Diabetes_binary', index=x_column, values=0)\n",
    "    return _df_plot\n",
    "\n",
    "fig, ax = plt.subplots(3, 4, figsize=(20, 20))\n",
    "axe = ax.ravel()\n",
    "c = len(cols)\n",
    "plt.suptitle('Diabetes Distribution by Features', fontsize=20)\n",
    "\n",
    "# Custom colors for the plot\n",
    "custom_colors = {'No Diabetes': 'green', 'Diabetes': 'red'}\n",
    "\n",
    "# Plotting each column in cols\n",
    "for i in range(c):\n",
    "    plot_data = create_plot_pivot(X, cols[i])\n",
    "    plot_data.plot(kind='bar', stacked=True, ax=axe[i], color=[custom_colors[val] for val in plot_data.columns])\n",
    "    axe[i].set_xlabel(cols[i])\n",
    "    \n",
    "    # Adding percentage labels for the \"Diabetes\" (red) part of each bar\n",
    "    for j, (index, row) in enumerate(plot_data.iterrows()):\n",
    "        total = row['No Diabetes'] + row['Diabetes']  # Sum of \"No Diabetes\" and \"Diabetes\" counts\n",
    "        if total > 0:\n",
    "            diabetes_percentage = (row['Diabetes'] / total) * 100  # Calculate percentage for Diabetes\n",
    "            axe[i].text(j, row['No Diabetes'] + row['Diabetes'] / 2, \n",
    "                        f\"{diabetes_percentage:.1f}%\", color=\"red\", ha=\"center\", va=\"top\")\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to fit the title\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter numeric columns only from X\n",
    "X_numeric = X.select_dtypes(include=[np.number])\n",
    "\n",
    "# Compute correlation with y\n",
    "X_numeric.corrwith(y['Diabetes_binary']).plot(kind='bar', grid=True, figsize=(20, 8),\n",
    "                                              title=\"Correlation with Diabetes_binary\", color=\"Purple\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "binary_features = ['HighBP', 'HighChol', 'CholCheck', 'Smoker', 'Stroke', 'HeartDiseaseorAttack', 'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'DiffWalk', 'Sex', 'Healthy_Diet', 'Mental_Health_Risk']\n",
    "continuous_features = ['BMI', 'MentHlth', 'PhysHlth', 'Age']\n",
    "ordinal_features = ['GenHlth', 'Education', 'Income', 'BMI_Category', 'Heart_Disease_Risk', 'Age_Group', 'Education_Level', 'Income_Group']\n",
    "\n",
    "def get_mutual_info(x_train, features, y_train, discrete=None):\n",
    "    if discrete == None:\n",
    "        mi_scores = mutual_info_classif(x_train[features], y_train['Diabetes_binary'])\n",
    "    else:\n",
    "        mi_scores = mutual_info_classif(x_train[features], y_train['Diabetes_binary'], discrete_features=discrete)\n",
    "    \n",
    "    return pd.DataFrame({'Features': features, 'Mutual Information Scores': mi_scores}).sort_values(by='Mutual Information Scores', ascending=False)\n",
    "\n",
    "print(get_mutual_info(X, binary_features, y, True))\n",
    "print()\n",
    "print(get_mutual_info(X, continuous_features, y))\n",
    "print()\n",
    "print(get_mutual_info(X, ordinal_features, y, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_probability_plot(data, col):\n",
    "    \"\"\"\n",
    "    Generates a normal probability plot for the given data.\n",
    "    \n",
    "    Parameters:\n",
    "        data (array-like): The data for which to generate the plot.\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Generate a probability plot\n",
    "    stats.probplot(data, dist=\"norm\", plot=plt)\n",
    "    plt.title(\"Normal Probability Plot for \"+ col)\n",
    "    plt.xlabel(\"Theoretical Quantiles\")\n",
    "    plt.ylabel(\"Sample Quantiles\")\n",
    "    plt.show()\n",
    "normal_probability_plot(X['BMI'], 'BMI')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_plot(data, target, columns):\n",
    "    \"\"\"\n",
    "    Generates a pair plot for the given data and specified columns.\n",
    "    \n",
    "    Parameters:\n",
    "        data (DataFrame): The data for which to generate the plot.\n",
    "        target (Series): The target variable for coloring.\n",
    "        columns (list): List of column names to include in the pair plot.\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Subset the data with the selected columns and target\n",
    "    data_with_target = data[columns].copy()\n",
    "    data_with_target['Diabetes_binary'] = target\n",
    "    data_with_target['Diabetes_binary'] = data_with_target['Diabetes_binary'].replace({0: 'No Diabetes', 1: 'Diabetes'})\n",
    "    palette = {'No Diabetes': 'blue', 'Diabetes': 'orange'}\n",
    "    # Generate the pair plot\n",
    "    sns.pairplot(\n",
    "        data=data_with_target,\n",
    "        hue='Diabetes_binary',\n",
    "        diag_kind='kde',  # KDE for diagonal\n",
    "        kind='None',       # KDE for off-diagonal\n",
    "        palette=palette\n",
    "    )\n",
    "\n",
    "\n",
    "for col in X.columns:\n",
    "    normal_probability_plot(X[col], col)\n",
    "\n",
    "\n",
    "#normal_probability_plot(X['BMI'], 'BMI')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation matrix\n",
    "X = X.drop(columns=['Age_Group'])\n",
    "corr = X.corr()\n",
    "plt.figure(figsize=(30, 15))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the data and Split validation set and train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import *\n",
    "is_equal_classes = False\n",
    "# Look for multicollinearity between the features and remove the features that are unnecessary\n",
    "# we already know the age so age_group is not needed\n",
    "X, y = get_training_data()\n",
    "X_testing = get_testing_data()\n",
    "X = X.drop('Unnamed: 0', axis=1 )\n",
    "x_testing_id = X_testing['Unnamed: 0']\n",
    "X_testing = X_testing.drop('Unnamed: 0', axis=1 )\n",
    "# Unnamed: 0 is the Id of the row, we can drop it\n",
    "y = y.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "columnsToDrop = ['Age_Group', 'MentHlth', 'HvyAlcoholConsump', 'NoDocbcCost', 'Smoker', 'Fruits', 'Veggies', 'Mental_Health_Risk' , 'AnyHealthcare' ] #'Heart_Disease_Risk', 'Income', 'Education'\n",
    "\n",
    "X = X.drop(columns=columnsToDrop)\n",
    "X = feature_encoding(X)\n",
    "\n",
    "X_testing = X_testing.drop(columns=columnsToDrop)\n",
    "X_testing = feature_encoding(X_testing)\n",
    "\n",
    "# Basic splits of the data\n",
    "X_train, X_validation, y_train, y_validation =  data_splits(X, y) #   split data\n",
    "is_equal_classes = False\n",
    "X_train_scaled, X_validation_scaled, X_testing_Scaled = normalize_features(X_train, X_validation, X_testing) #   normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Equal classes splits of the data\n",
    "# X_train, X_validation, y_train, y_validation =  data_splits_equal_classes(X, y) #   split data into equal classes. Meaning there is as many diabetes as non diabetes. But we remove some data\n",
    "# is_equal_classes = True\n",
    "# X_train_scaled, X_validation_scaled, X_testing_Scaled = normalize_features(X_train, X_validation, X_testing) #   normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import gen_batches\n",
    "def predict_in_batches(cls, X, batch_size=100):\n",
    "    \"\"\"\n",
    "    Make predictions for the input data in batches.\n",
    "    \n",
    "    Parameters:\n",
    "        cls (object): The trained classifier model.\n",
    "        X (array-like): The input data to make predictions for.\n",
    "        batch_size (int): The size of each batch.\n",
    "        \n",
    "    Returns:\n",
    "        array-like: The predictions for the input data.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    # Generate batches and make predictions for each batch\n",
    "    for batch in gen_batches(len(X), batch_size):\n",
    "        batch_predictions = cls.predict(X[batch])\n",
    "        predictions.append(batch_predictions)\n",
    "    \n",
    "    # Concatenate all batch predictions into a single array\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "def perform_grid_search(model, X_train_scaled, y_train, params):\n",
    "    print(\"Performing grid search for \", model)\n",
    "    if not isFinal:\n",
    "        # Define the cross-validation strategy\n",
    "        strat_kfold = StratifiedKFold(n_splits=10) # TODO\n",
    "\n",
    "        # Grid search for the model \n",
    "        #\"f1_macro\": Calculates F1-score per class and takes the average, treating all classes equally.\n",
    "        #\"f1_weighted\": Calculates F1-score per class and takes a weighted average, considering class imbalance.\n",
    "        grid_search = GridSearchCV(model, params, scoring='f1', cv=strat_kfold, n_jobs=10) # n_jobs=10 uses 10 parallel processes. Speeds up the process \n",
    "\n",
    "\n",
    "        if isinstance(model, RandomForestClassifier) or isinstance(model, SVC):\n",
    "            grid_search.fit(X_train_scaled, y_train.values.ravel())\n",
    "        else:\n",
    "            grid_search.fit(X_train, y_train)\n",
    "        best_param = grid_search.best_params_\n",
    "        best_score = grid_search.best_score_ \n",
    "        print(\"Best parameters are:\", best_param)\n",
    "        print(\"Best score is:\", best_score)\n",
    "\n",
    "        # Return the fitted grid search objects\n",
    "        return grid_search, best_param, best_score\n",
    "    else:\n",
    "        print(\"Skipping grid search for TA evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(model, X_train_scaled, y_train, X_validation_scaled, y_validation):\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_validation_pred = model.predict(X_validation_scaled)\n",
    "    validation_f1 = f1_score(y_validation, y_validation_pred)\n",
    "    print(\"Validation f1 score:\", validation_f1)\n",
    "    print(classification_report(y_validation, y_validation_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_summary(y_train, y_train_pred, y_validation, y_validation_pred):\n",
    "    train_f1 = f1_score(y_train, y_train_pred) \n",
    "    validation_f1 = f1_score(y_validation, y_validation_pred) \n",
    "    print(\"Train f1 score:\", train_f1)\n",
    "\n",
    "    print(\"Validation f1 score:\", validation_f1)\n",
    "    print(\"\\nTraining Set Classification Report:\")\n",
    "\n",
    "    print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "    print(\"Training Set Confusion Matrix:\")\n",
    "\n",
    "    print(confusion_matrix(y_train, y_train_pred))\n",
    "\n",
    "    print(\"\\nValidation Set Classification Report:\")\n",
    "\n",
    "    print(classification_report(y_validation, y_validation_pred))\n",
    "\n",
    "    print(\"Validation Set Confusion Matrix:\")\n",
    "\n",
    "    print(confusion_matrix(y_validation, y_validation_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_summary_one( y_validation, y_validation_pred):\n",
    "    validation_f1 = f1_score(y_validation, y_validation_pred) \n",
    "    print(\"Validation f1 score:\", validation_f1)\n",
    "    print(\"\\nValidation Set Classification Report:\")\n",
    "\n",
    "    print(classification_report(y_validation, y_validation_pred))\n",
    "\n",
    "    print(\"Validation Set Confusion Matrix:\")\n",
    "\n",
    "    print(confusion_matrix(y_validation, y_validation_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaggle_submission(model, X_testing_scaled, x_testing_id):\n",
    "    y_pred = model.predict(X_testing_scaled)\n",
    "    y_pred = pd.DataFrame({\n",
    "        'index': x_testing_id , \n",
    "        'Diabetes_binary': y_pred,\n",
    "    })\n",
    "\n",
    "    # Save the predictions to a CSV file\n",
    "    y_pred.to_csv(f'./{model}_y_pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission_pred(model, X_testing_scaled):\n",
    "    return model.predict(X_testing_scaled)\n",
    "\n",
    "\n",
    "def submission_proba(model, X_testing_scaled):\n",
    "    return model.predict_proba(X_testing_scaled)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### TESTING ####################\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "\n",
    "cls_randomforest = RandomForestClassifier(class_weight='balanced', \n",
    "                                          random_state=42, \n",
    "                                          max_depth=20, \n",
    "                                          max_leaf_nodes=200, \n",
    "                                          min_samples_leaf=20, n_estimators=500)\n",
    "\n",
    "param_grid_random_forest = {\n",
    "    'n_estimators': [10, 200, 300, 400, 500, 1000, 5000],\n",
    "    'max_depth': [20, 30],\n",
    "    'bootstrap': [True, False],\n",
    "}\n",
    "if not isFinal:\n",
    "    perform_grid_search(cls_randomforest, X_train_scaled, y_train, params= param_grid_random_forest)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_equal_classes:\n",
    "    print('Equal classes')\n",
    "    cls_randomforest = RandomForestClassifier(class_weight='balanced',\n",
    "                                          random_state=42, \n",
    "                                          max_depth=20, \n",
    "                                          max_leaf_nodes=200, \n",
    "                                           n_estimators=1000) #min_samples_leaf=20\n",
    "else:\n",
    "    cls_randomforest = RandomForestClassifier( class_weight={0: 1, 1: 3}, # class_weight='balanced',\n",
    "                                            random_state=0, \n",
    "                                            max_depth=20, \n",
    "                                            max_leaf_nodes=200, \n",
    "                                            n_estimators=1000) #min_samples_leaf=20\n",
    "\n",
    "model_training(cls_randomforest, X_train_scaled, y_train, X_validation_scaled, y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=12)#n_components=2 #\n",
    "pca.fit(X_train_scaled)\n",
    "X_train_pca = pca.transform(X_train_scaled)\n",
    "cls_randomforest_pca = RandomForestClassifier(class_weight='balanced', random_state=42, max_depth=20, max_leaf_nodes=200, n_estimators=1000)\n",
    "model_training(cls_randomforest_pca, X_train_pca, y_train, pca.transform(X_validation_scaled), y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "cls_decision_tree = DecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "\n",
    "param_grid_decision_tree = {\n",
    "    'criterion': ['gini','entropy'],\n",
    "    'max_depth': [10, 20, 50],\n",
    "    'min_samples_leaf': [2, 3, 10],\n",
    "    'max_leaf_nodes': [5, 10, 50]\n",
    "}\n",
    "\n",
    "perform_grid_search(cls_decision_tree, X_train_scaled, y_train, params=param_grid_decision_tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_decision_tree = DecisionTreeClassifier(criterion='gini', class_weight={0: 1, 1: 3}, max_depth=10, min_samples_leaf=2, max_leaf_nodes=10)\n",
    "model_training(cls_decision_tree, X_train_scaled, y_train, X_validation_scaled, y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree with Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "cls_decision_tree = DecisionTreeClassifier(class_weight={0: 1, 1: 3}, criterion='gini', max_depth=10, min_samples_leaf=2, max_leaf_nodes=10)\n",
    "bagging_model = BaggingClassifier(estimator=cls_decision_tree, n_estimators=50, random_state=42)\n",
    "model_training(bagging_model, X_train_scaled, y_train, X_validation_scaled, y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree with Adaboosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Set up a decision stump as the weak learner\n",
    "base_estimator = DecisionTreeClassifier(max_depth=1, class_weight={0: 1, 1: 3}, criterion='gini')  # Decision stump class_weight='balanced'\n",
    "\n",
    "# Create an AdaBoost classifier using the decision tree as the base estimator\n",
    "ada_boost = AdaBoostClassifier(estimator=base_estimator, n_estimators=50, learning_rate=1.0, random_state=42)\n",
    "\n",
    "# Assuming `model_training` is a function that trains and evaluates the model\n",
    "model_training(ada_boost, X_train_scaled, y_train, X_validation_scaled, y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_logistic = LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000, dual=False, C=5)\n",
    "param_grid_logistic = {\n",
    "    'max_iter':[1000, 2000, 3000],\n",
    "    'tol': [1e-3, 1e-4, 1e-5],\n",
    "    'C': [0.5, 1, 10],\n",
    "}\n",
    "if not isFinal:\n",
    "    perform_grid_search(cls_logistic, X_train_scaled, y_train, param_grid_logistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use best parameters to train the model\n",
    "if is_equal_classes:\n",
    "    print('Equal classes')\n",
    "    cls_logistic = LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000, dual=False, C=1, tol=1e-3)\n",
    "else:\n",
    "    cls_logistic = LogisticRegression(random_state=42, class_weight={0: 1, 1: 4}, max_iter=1000, dual=False, C=1, tol=1e-3)\n",
    "model_training(cls_logistic, X_train_scaled, y_train, X_validation_scaled, y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with Adaboosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logistic regression as the base estimator\n",
    "cls_logistic_regiression = LogisticRegression(random_state=42, class_weight={0: 1, 1: 4}, max_iter=1000, dual=False, C=1, tol=1e-3)\n",
    "\n",
    "# Create an AdaBoost classifier using logistic regression as the base estimator\n",
    "ada_boost = AdaBoostClassifier(estimator=cls_logistic_regiression, n_estimators=50, learning_rate=1.0, random_state=42)\n",
    "\n",
    "model_training(ada_boost, X_train_scaled, y_train, X_validation_scaled, y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the XGBoost classifier\n",
    "# booster [default= gbtree ]\n",
    "# Which booster to use. Can be gbtree, gblinear or dart; gbtree and dart use tree based models while gblinear uses linear functions.\n",
    "xgb_model = XGBClassifier(random_state=42, objective='binary:logistic', eval_metric='logloss') # binary:hinge\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'eval_metric': ['error', 'logloss'],\n",
    "    'booster': ['gbtree', 'gblinear'],\n",
    "    'eval_metric': ['error'],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7, 15]\n",
    "}\n",
    "perform_grid_search(xgb_model, X_train_scaled, y_train, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier(random_state=0, max_depth=10,  objective='binary:logistic', eval_metric='error', booster='gbtree', n_estimators=5000)\n",
    "model_training(xgb_model, X_train_scaled, y_train, X_validation_scaled, y_validation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_svm = SVC(class_weight='balanced', random_state=42, kernel='rbf', C=1)\n",
    "param_grid_svm = {\n",
    "    'kernel':['rbf', 'sigmoid'],\n",
    "    'shrinking': [True],\n",
    "    'C': [1, 10],\n",
    "}\n",
    "perform_grid_search(cls_svm, X_train_scaled, y_train, param_grid_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_svm = SVC(class_weight='balanced', random_state=42, kernel='rbf', C=1, shrinking=True)\n",
    "model_training(cls_svm, X_train_scaled, y_train, X_validation_scaled, y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "param_grid_svm = {\n",
    "    'weights':['distance', 'uniform', None],\n",
    "    'leaf_size': [10, 30, 50],\n",
    "}\n",
    "cls_knn = KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto')\n",
    "perform_grid_search(cls_knn, X_train_scaled, y_train, param_grid_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_knn = KNeighborsClassifier(n_neighbors=10,  weights='distance', algorithm='kd_tree', leaf_size= 10)\n",
    "model_training(cls_knn, X_train_scaled, y_train, X_validation_scaled, y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_mlp = MLPClassifier( random_state=42, kernel='rbf', C=1)\n",
    "param_grid_mlp = {\n",
    "    'hidden_layer_sizes':[(100, 50), (50, 50), (100, 100)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "    'learning_rate': ['constant', 'adaptive'],\n",
    "    'max_iter': [200, 300, 400],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'shuffle': [True, False],\n",
    "}\n",
    "perform_grid_search(cls_mlp, X_train_scaled, y_train, param_grid_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_mlp = MLPClassifier(random_state=42, max_iter=1000, hidden_layer_sizes=(100, 50, 10, 1), activation='relu', solver='adam', alpha=0.001, learning_rate='adaptive', shuffle=True)\n",
    "cls_mlp.fit(X_train_scaled, y_train)\n",
    "y_validation_pred = cls_mlp.predict(X_validation_scaled)\n",
    "classification_summary_one(y_validation, y_validation_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation set predictions\n",
    "\n",
    "pred_array = []\n",
    "for model in [cls_randomforest, cls_logistic, bagging_model, ada_boost]:#xgb_model\n",
    "    pred_array.append(submission_pred(model, X_validation_scaled)) \n",
    "\n",
    "# Take the majority vote\n",
    "pred_array = np.array(pred_array)\n",
    "for i in range(len(pred_array[0])):\n",
    "    if sum([pred[i] for pred in pred_array]) >= pred_array.shape[0] / 2:\n",
    "        pred_array[0][i] = 1\n",
    "    else:\n",
    "        pred_array[0][i] = 0\n",
    "\n",
    "classification_summary_one(y_validation, pred_array[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions for multiple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing set predictions\n",
    "\n",
    "pred_array = []\n",
    "for model in [cls_randomforest, cls_logistic, bagging_model, ada_boost]:#xgb_model\n",
    "    pred_array.append(submission_pred(model, X_testing_Scaled))\n",
    "\n",
    "# Take the majority vote\n",
    "pred_array = np.array(pred_array)\n",
    "for i in range(len(pred_array[0])):\n",
    "    if sum([pred[i] for pred in pred_array]) >= pred_array.shape[0] / 2:\n",
    "        pred_array[0][i] = 1\n",
    "    else:\n",
    "        pred_array[0][i] = 0\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "y_pred = pd.DataFrame({\n",
    "    'index': x_testing_id , \n",
    "    'Diabetes_binary': pred_array[0],\n",
    "})\n",
    "if not isFinal:\n",
    "    y_pred.to_csv('./y_pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_array = []\n",
    "for model in [cls_randomforest, cls_logistic, xgb_model, bagging_model, ada_boost]:\n",
    "    prob_array.append(submission_proba(model, X_validation_scaled))\n",
    "\n",
    "# Take most confident prediction\n",
    "prob_array = np.array(prob_array)\n",
    "for i in range(len(prob_array[0])):\n",
    "    if sum([prob[i] for prob in prob_array]) >= 2.3: # best is either 1.3 or 1.4 considering 3 models\n",
    "        prob_array[0][i] = 1\n",
    "    else:\n",
    "        prob_array[0][i] = 0\n",
    "pred_array = prob_array[0].astype(int)\n",
    "classification_summary_one(y_validation, pred_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_array = []\n",
    "for model in [cls_randomforest, cls_logistic, xgb_model, bagging_model, ada_boost]:\n",
    "    pred_array.append(submission_pred(model, X_validation_scaled))\n",
    "\n",
    "prob_array = []\n",
    "for model in [cls_randomforest, cls_logistic, xgb_model, bagging_model, ada_boost]:\n",
    "    prob_array.append(submission_proba(model, X_validation_scaled))\n",
    "\n",
    "# Take the majority vote\n",
    "pred_array = np.array(pred_array)\n",
    "for i in range(len(pred_array[0])):\n",
    "    if sum([pred[i] for pred in pred_array]) >= pred_array.shape[0] / 2 or sum([prob[i] for prob in prob_array]) >= 3.0: # logistic has been train on a equal class dataset. This is an experiment\n",
    "        pred_array[0][i] = 1\n",
    "    else:\n",
    "        pred_array[0][i] = 0\n",
    "\n",
    "classification_summary_one(y_validation, pred_array[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging + PCA models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_array = []\n",
    "\n",
    "for model in [cls_randomforest, cls_logistic, bagging_model, ada_boost]:#xgb_model\n",
    "    pred_array.append(submission_pred(model, X_validation_scaled))\n",
    "\n",
    "for pca_model in [cls_randomforest_pca]:\n",
    "    pred_array.append(submission_proba(pca_model, pca.transform(X_validation_scaled))) # 16 features. 12 components\n",
    "\n",
    "# Take the majority vote\n",
    "pred_array = np.array(pred_array)\n",
    "for i in range(len(pred_array[0])):\n",
    "    if sum([pred[i] for pred in pred_array]) >= pred_array.shape[0] / 2:\n",
    "        pred_array[0][i] = 1\n",
    "    else:\n",
    "        pred_array[0][i] = 0\n",
    "\n",
    "classification_summary_one(y_validation, pred_array[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss overtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loss overtime \n",
    "def loss_overtime(train_loss_array, validation_loss_array, F1_score_array, model_name):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    if len(train_loss_array) != 0:\n",
    "        plt.plot(train_loss_array, label='Train Loss')\n",
    "    if len(validation_loss_array) != 0:\n",
    "        plt.plot(validation_loss_array, label='Validation Loss')    \n",
    "    if len(F1_score_array) != 0:\n",
    "        plt.plot(F1_score_array, label='F1 Score')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.xticks(range(0, len(train_loss_array), 10))\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title(f'Loss over time for {model_name}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "# Ensure you have access to a GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Step 1: Calculate Class Weights\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.array([0, 1]), y=y_train['Diabetes_binary'].values.ravel())\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "# Step 2: Define the Custom Neural Network\n",
    "class CustomNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(CustomNN, self).__init__()\n",
    "        # Adding more layers and neurons\n",
    "        print('input size',input_size)\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.fc5 = nn.Linear(32, 16)\n",
    "        self.output = nn.Linear(16, 1)  # Binary classification output layer\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        # Xavier uniform initialization\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.xavier_uniform_(self.fc3.weight)\n",
    "        nn.init.xavier_uniform_(self.fc4.weight)\n",
    "        nn.init.xavier_uniform_(self.output.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# Step 3: Initialize Model, Loss Function, and Optimizer\n",
    "\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).to(device)\n",
    "\n",
    "X_validation_tensor = torch.tensor(X_validation_scaled, dtype=torch.float32).to(device)\n",
    "y_validation_tensor = torch.tensor(y_validation.values, dtype=torch.float32).to(device)\n",
    "\n",
    "input_size = X_train_tensor.shape[1]  # Replace with the actual number of input features\n",
    "model = CustomNN(input_size).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])  # Use pos_weight for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "# Step 4: Training Loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(X_train_tensor).squeeze()\n",
    "    loss = criterion(outputs, y_train_tensor.squeeze())\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation after each epoch\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_validation_tensor).squeeze()\n",
    "            val_loss = criterion(val_outputs, y_validation_tensor.squeeze())\n",
    "            val_preds = torch.round(torch.sigmoid(val_outputs))\n",
    "            \n",
    "            # Calculate F1 Score or other metrics\n",
    "            # (Your F1 calculation code here using val_preds and y_validation_tensor)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}, Validation Loss: {val_loss.item()},  F1 Score: {f1}\")\n",
    "classification_summary_one(y_validation, val_preds.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network with normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Ensure you have access to a GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Step 1: Calculate Class Weights\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.array([0, 1]), y=y_train['Diabetes_binary'].values.ravel())\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "# Step 2: Define the Custom Neural Network\n",
    "class CustomNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(CustomNN, self).__init__()\n",
    "        # Adding more layers and neurons\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)  # Batch normalization\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)  # Batch normalization\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)   # Batch normalization\n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.bn4 = nn.BatchNorm1d(32)  # Batch normalization\n",
    "        self.fc5 = nn.Linear(32, 16)\n",
    "        self.bn5 = nn.BatchNorm1d(16)  # Batch normalization\n",
    "        self.output = nn.Linear(16, 1)  # Binary classification output layer\n",
    "\n",
    "        # self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        # Xavier uniform initialization\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.xavier_uniform_(self.fc3.weight)\n",
    "        nn.init.xavier_uniform_(self.fc4.weight)\n",
    "        nn.init.xavier_uniform_(self.output.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn1(F.leaky_relu(self.fc1(x)))\n",
    "        # x = self.dropout(x)\n",
    "        x = self.bn2(F.leaky_relu(self.fc2(x)))\n",
    "        # x = self.dropout(x)\n",
    "        x = self.bn3(F.leaky_relu(self.fc3(x)))\n",
    "        # x = self.dropout(x)\n",
    "        x = self.bn4(F.leaky_relu(self.fc4(x)))\n",
    "        # x = self.dropout(x)\n",
    "        x = self.bn5(F.leaky_relu(self.fc5(x)))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# Step 3: Initialize Model, Loss Function, and Optimizer\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).to(device)\n",
    "\n",
    "X_validation_tensor = torch.tensor(X_validation_scaled, dtype=torch.float32).to(device)\n",
    "y_validation_tensor = torch.tensor(y_validation.values, dtype=torch.float32).to(device)\n",
    "\n",
    "input_size = X_train_tensor.shape[1]  # Replace with the actual number of input features\n",
    "model = CustomNN(input_size).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])  # Use pos_weight for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Step 4: Training Loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(X_train_tensor).squeeze()\n",
    "    loss = criterion(outputs, y_train_tensor.squeeze())\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation after each epoch\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_validation_tensor).squeeze()\n",
    "            val_loss = criterion(val_outputs, y_validation_tensor.squeeze())\n",
    "            val_preds = torch.round(torch.sigmoid(val_outputs))\n",
    "            \n",
    "            # Calculate F1 Score or other metrics\n",
    "            # (Your F1 calculation code here using val_preds and y_validation_tensor)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}, Validation Loss: {val_loss.item()},  F1 Score: {f1}\")\n",
    "classification_summary_one(y_validation, val_preds.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"Is CUDA available?\", torch.cuda.is_available())\n",
    "print(\"Available devices:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Current CUDA device:\", torch.cuda.current_device())\n",
    "    print(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Ensure you have access to a GPU if available\n",
    "print(\"Is CUDA available?\", torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_loss_array = []\n",
    "validation_loss_array = []\n",
    "F1_score_array = []\n",
    "\n",
    "# Step 1: Calculate Class Weights\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.array([0, 1]), y=y_train['Diabetes_binary'].values.ravel())\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "# Step 2: Define the Custom Neural Network\n",
    "class CustomNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(CustomNN, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(input_size, 512),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.Linear(32, 16)\n",
    "        ])\n",
    "        self.output = nn.Linear(16, 1)  # Binary classification output layer\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        # Xavier uniform initialization\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.Linear):  # Ensure it's a linear layer\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.output.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = F.leaky_relu(layer(x))  # Apply ReLU after each linear layer\n",
    "            x = self.dropout(x)  # Apply dropout after activation\n",
    "        x = self.output(x)  # Final output layer (no activation here; handled by BCEWithLogitsLoss)\n",
    "        return x\n",
    "\n",
    "# Step 3: Initialize Model, Loss Function, and Optimizer\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).to(device)\n",
    "\n",
    "X_validation_tensor = torch.tensor(X_validation_scaled, dtype=torch.float32).to(device)\n",
    "y_validation_tensor = torch.tensor(y_validation.values, dtype=torch.float32).to(device)\n",
    "\n",
    "input_size = X_train_tensor.shape[1]\n",
    "model = CustomNN(input_size).to(device)\n",
    "weights = torch.tensor(2.5, device='cuda:0')\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])  # Use pos_weight for binary classification # \n",
    "optimizer = optim.NAdam(model.parameters(), lr=0.005)\n",
    "\n",
    "# Step 4: Training Loop\n",
    "num_epochs = 100  # Increased epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(X_train_tensor).squeeze()\n",
    "    loss = criterion(outputs, y_train_tensor.squeeze())\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_validation_tensor).squeeze()\n",
    "        val_loss = criterion(val_outputs, y_validation_tensor.squeeze())\n",
    "        val_preds = torch.round(torch.sigmoid(val_outputs))\n",
    "            \n",
    "            # Calculate F1 Score or other metrics\n",
    "            # (Your F1 calculation code here using val_preds and y_validation_tensor)\n",
    "        f1 = f1_score(y_validation_tensor.cpu().numpy(), val_preds.cpu().numpy())    \n",
    "\n",
    "    # Validation after each epoch\n",
    "    train_loss_array.append(loss.item())\n",
    "    validation_loss_array.append(val_loss.item())\n",
    "    F1_score_array.append(f1)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}, Validation Loss: {val_loss.item()},  F1 Score: {f1}\")\n",
    "\n",
    "classification_summary_one(y_validation, val_preds.cpu().numpy())\n",
    "loss_overtime(train_loss_array, validation_loss_array, F1_score_array, 'Neural Network')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks Expiriment with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not isFinal:\n",
    "\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "    from sklearn.metrics import f1_score\n",
    "    import numpy as np\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "    # Ensure you have access to a GPU if available\n",
    "    print(\"Is CUDA available?\", torch.cuda.is_available())\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Apply SMOTE to the training data\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train['Diabetes_binary'])\n",
    "\n",
    "    # Ensure resampled data is converted to NumPy arrays\n",
    "    X_train_resampled = np.array(X_train_resampled)\n",
    "    y_train_resampled = np.array(y_train_resampled)\n",
    "\n",
    "    # Update tensors for the resampled data\n",
    "    X_train_tensor = torch.tensor(X_train_resampled, dtype=torch.float32).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train_resampled, dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "    # Step 1: Calculate Class Weights (optional after SMOTE, since it balances the data)\n",
    "    # If you still want to calculate class weights:\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight='balanced', \n",
    "        classes=np.array([0, 1]), \n",
    "        y=y_train_resampled\n",
    "    )\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Step 2: Define the Custom Neural Network (no change required here)\n",
    "    class CustomNN(nn.Module):\n",
    "        def __init__(self, input_size):\n",
    "            super(CustomNN, self).__init__()\n",
    "            self.layers = nn.ModuleList([\n",
    "                nn.Linear(input_size, 512),\n",
    "                nn.Linear(512, 512),\n",
    "                nn.Linear(512, 256),\n",
    "                nn.Linear(256, 256),\n",
    "                nn.Linear(256, 128),\n",
    "                nn.Linear(128, 128),\n",
    "                nn.Linear(128, 64),\n",
    "                nn.Linear(64, 64),\n",
    "                nn.Linear(64, 32),\n",
    "                nn.Linear(32, 16)\n",
    "            ])\n",
    "            self.output = nn.Linear(16, 1)  # Binary classification output layer\n",
    "            self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "            # Xavier uniform initialization\n",
    "            for layer in self.layers:\n",
    "                if isinstance(layer, nn.Linear):  # Ensure it's a linear layer\n",
    "                    nn.init.xavier_uniform_(layer.weight)\n",
    "\n",
    "            nn.init.xavier_uniform_(self.output.weight)\n",
    "\n",
    "        def forward(self, x):\n",
    "            for layer in self.layers:\n",
    "                x = F.leaky_relu(layer(x))  # Apply ReLU after each linear layer\n",
    "                x = self.dropout(x)  # Apply dropout after activation\n",
    "            x = self.output(x)  # Final output layer (no activation here; handled by BCEWithLogitsLoss)\n",
    "            return x\n",
    "\n",
    "    # Step 3: Initialize Model, Loss Function, and Optimizer\n",
    "    X_validation_tensor = torch.tensor(X_validation_scaled, dtype=torch.float32).to(device)\n",
    "    y_validation_tensor = torch.tensor(y_validation.values, dtype=torch.float32).to(device)\n",
    "\n",
    "    input_size = X_train_tensor.shape[1]\n",
    "    model = CustomNN(input_size).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])  # Use pos_weight for binary classification\n",
    "    optimizer = optim.NAdam(model.parameters(), lr=0.005)\n",
    "\n",
    "    # Step 4: Training Loop\n",
    "    num_epochs = 100  # Increased epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(X_train_tensor).squeeze()\n",
    "        loss = criterion(outputs, y_train_tensor.squeeze())\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validation after each epoch\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_outputs = model(X_validation_tensor).squeeze()\n",
    "                val_loss = criterion(val_outputs, y_validation_tensor.squeeze())\n",
    "                val_preds = torch.round(torch.sigmoid(val_outputs))\n",
    "\n",
    "                # Calculate F1 Score\n",
    "                f1 = f1_score(y_validation_tensor.cpu().numpy(), val_preds.cpu().numpy())\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}, Validation Loss: {val_loss.item()}, F1 Score: {f1}\")\n",
    "\n",
    "    classification_summary_one(y_validation, val_preds.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not isFinal:\n",
    "\n",
    "    import torch\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    import numpy as np\n",
    "\n",
    "    # Apply SMOTE on the training data (X_train and y_train)\n",
    "    smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train['Diabetes_binary'].values)\n",
    "\n",
    "    # Convert resampled data to PyTorch tensors\n",
    "    X_train_resampled_tensor = torch.tensor(X_train_resampled, dtype=torch.float32).to(device)\n",
    "    y_train_resampled_tensor = torch.tensor(y_train_resampled, dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "    class FocalLoss(nn.Module):\n",
    "        def __init__(self, alpha=0.25, gamma=2, size_average=True):\n",
    "            super(FocalLoss, self).__init__()\n",
    "            self.alpha = alpha\n",
    "            self.gamma = gamma\n",
    "            self.size_average = size_average\n",
    "\n",
    "        def forward(self, inputs, targets):\n",
    "            # Apply sigmoid to the model's output to get probabilities\n",
    "            inputs = torch.sigmoid(inputs)\n",
    "            \n",
    "            # Calculate cross-entropy\n",
    "            BCE_loss = F.binary_cross_entropy(inputs, targets, reduction='none')\n",
    "\n",
    "            # Calculate focal loss components\n",
    "            pt = torch.exp(-BCE_loss)  # pt = probability of correct class\n",
    "            focal_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n",
    "\n",
    "            # Return the average loss\n",
    "            if self.size_average:\n",
    "                return torch.mean(focal_loss)\n",
    "            else:\n",
    "                return torch.sum(focal_loss)\n",
    "\n",
    "    # Use FocalLoss in the training loop:\n",
    "    focal_loss = FocalLoss(alpha=0.25, gamma=2).to(device)\n",
    "    # Initialize FocalLoss in place of BCEWithLogitsLoss\n",
    "    criterion = FocalLoss(alpha=0.25, gamma=2).to(device)\n",
    "\n",
    "    # Training Loop\n",
    "    num_epochs = 100\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(X_train_resampled_tensor).squeeze()\n",
    "        loss = criterion(outputs, y_train_resampled_tensor.squeeze())\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validation after each epoch\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_outputs = model(X_validation_tensor).squeeze()\n",
    "                val_loss = criterion(val_outputs, y_validation_tensor.squeeze())\n",
    "                val_preds = torch.round(torch.sigmoid(val_outputs))\n",
    "\n",
    "                # Calculate F1 Score or other metrics\n",
    "                classification_summary_one(y_validation, val_preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Ensure that X_testing_Scaled is a tensor\n",
    "X_testing_Scaled_tensor = torch.tensor(X_testing_Scaled, dtype=torch.float32).to(device)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# No gradient calculation needed during inference\n",
    "with torch.no_grad():\n",
    "    # Get the model's predictions\n",
    "    predictions = model(X_testing_Scaled_tensor).squeeze()  # Squeeze to remove any extra dimensions\n",
    "\n",
    "    # Apply sigmoid activation function for binary classification\n",
    "    predictions = torch.sigmoid(predictions)\n",
    "\n",
    "    # Convert the probabilities to binary predictions (0 or 1)\n",
    "    predictions_binary = torch.round(predictions)\n",
    "\n",
    "\n",
    "# If you want to convert predictions back to a NumPy array for easier use:\n",
    "predictions_binary = predictions_binary.cpu().numpy()\n",
    "\n",
    "# Now you have predictions for the test data (0 or 1)\n",
    "y_pred_nn = pd.DataFrame({\n",
    "    'index': x_testing_id , \n",
    "    'Diabetes_binary': predictions_binary,\n",
    "})\n",
    "y_pred_nn.to_csv('./y_pred_nn.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference between Random forest and NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = submission_pred(cls_randomforest, X_testing_Scaled)\n",
    "y_pred = pd.DataFrame({\n",
    "    'index': x_testing_id , \n",
    "    'Diabetes_binary': y_pred,\n",
    "})\n",
    "# Reset indices to make sure both DataFrames are aligned\n",
    "y_pred_nn = y_pred_nn.reset_index(drop=True)\n",
    "y_pred = y_pred.reset_index(drop=True)\n",
    "\n",
    "# Now you can compare the predictions without worrying about index misalignment\n",
    "diff_counter = 0\n",
    "for index in range(len(y_pred_nn)):\n",
    "    if y_pred_nn.loc[index, 'Diabetes_binary'] != y_pred.loc[index, 'Diabetes_binary']:\n",
    "        diff_counter += 1\n",
    "\n",
    "print('Number of differences:', diff_counter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions of multiple model and neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation set predictions\n",
    "X_testing_Scaled_tensor = torch.tensor(X_validation_scaled, dtype=torch.float32).to(device)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# No gradient calculation needed during inference\n",
    "with torch.no_grad():\n",
    "    # Get the model's predictions\n",
    "    predictions = model(X_testing_Scaled_tensor).squeeze()  # Squeeze to remove any extra dimensions\n",
    "\n",
    "    # Apply sigmoid activation function for binary classification\n",
    "    predictions = torch.sigmoid(predictions)\n",
    "\n",
    "    # Convert the probabilities to binary predictions (0 or 1)\n",
    "    predictions_binary = torch.round(predictions)\n",
    "\n",
    "\n",
    "# If you want to convert predictions back to a NumPy array for easier use:\n",
    "predictions_binary = predictions_binary.cpu().numpy()\n",
    "################################################\n",
    "pred_array = []\n",
    "pred_array.append(submission_pred(cls_randomforest, X_validation_scaled))\n",
    "pred_array.append(submission_pred(cls_logistic, X_validation_scaled))\n",
    "# pred_array.append(submission_pred(ada_boost, X_validation_scaled))\n",
    "pred_array.append(predictions_binary)\n",
    "\n",
    "# Take the majority vote\n",
    "pred_array = np.array(pred_array)\n",
    "for i in range(len(pred_array[0])):\n",
    "    if sum([pred[i] for pred in pred_array]) >= pred_array.shape[0] / 2:\n",
    "        pred_array[0][i] = 1\n",
    "    else:\n",
    "        pred_array[0][i] = 0\n",
    "\n",
    "classification_summary_one(y_validation, pred_array[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
