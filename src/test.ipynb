{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "from data_processing import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_training_data()\n",
    "X_testing = get_testing_data()\n",
    "#Unnamed: 0 is the Id of the row, we can drop it\n",
    "X = X.drop('Unnamed: 0', axis=1 )\n",
    "\n",
    "x_testing_id = X_testing['Unnamed: 0']\n",
    "X_testing = X_testing.drop('Unnamed: 0', axis=1 )\n",
    "# Unnamed: 0 is the Id of the row, we can drop it\n",
    "y = y.drop(columns=['Unnamed: 0'])  # If the first column is labeled as 'Unnamed: 0'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of elements :', len(X))\n",
    "print('Number of features :', len(X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('len(y):', len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = {}\n",
    "for col in X.columns:\n",
    "    unique_values[col] = X[col].value_counts().shape[0]\n",
    "\n",
    "pd.DataFrame(unique_values, index=['unique value count']).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,25))\n",
    "for i,col in enumerate(['BMI', 'GenHlth', 'MentHlth', 'PhysHlth', 'Age','Education', 'Income']):\n",
    "    plt.subplot(4,2,i+1)\n",
    "    plt.title('Distribution of '+col)\n",
    "    plt.boxplot(x = col, data = X, vert=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['HighBP', 'HighChol', 'CholCheck', 'Smoker', 'Stroke', 'HeartDiseaseorAttack', \n",
    "        'PhysActivity', 'Veggies', 'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'DiffWalk']\n",
    "\n",
    "def create_plot_pivot(data, x_column):\n",
    "    \"\"\"Create a pivot table for satisfaction versus another rating for easy plotting.\"\"\"\n",
    "    # Merge `X` and `y` to ensure access to 'Diabetes_binary' column for grouping\n",
    "    data_with_target = data.copy()\n",
    "    data_with_target['Diabetes_binary'] = y['Diabetes_binary']\n",
    "    data_with_target['Diabetes_binary'] = data_with_target['Diabetes_binary'].replace({0: 'No Diabetes', 1: 'Diabetes'})\n",
    "    \n",
    "    _df_plot = data_with_target.groupby([x_column, 'Diabetes_binary']).size() \\\n",
    "                   .reset_index().pivot(columns='Diabetes_binary', index=x_column, values=0)\n",
    "    return _df_plot\n",
    "\n",
    "fig, ax = plt.subplots(3, 4, figsize=(20, 20))\n",
    "axe = ax.ravel()\n",
    "c = len(cols)\n",
    "plt.suptitle('Diabetes Distribution by Features', fontsize=20)\n",
    "\n",
    "# Custom colors for the plot\n",
    "custom_colors = {'No Diabetes': 'green', 'Diabetes': 'red'}\n",
    "\n",
    "# Plotting each column in cols\n",
    "for i in range(c):\n",
    "    plot_data = create_plot_pivot(X, cols[i])\n",
    "    plot_data.plot(kind='bar', stacked=True, ax=axe[i], color=[custom_colors[val] for val in plot_data.columns])\n",
    "    axe[i].set_xlabel(cols[i])\n",
    "    \n",
    "    # Adding percentage labels for the \"Diabetes\" (red) part of each bar\n",
    "    for j, (index, row) in enumerate(plot_data.iterrows()):\n",
    "        total = row['No Diabetes'] + row['Diabetes']  # Sum of \"No Diabetes\" and \"Diabetes\" counts\n",
    "        if total > 0:\n",
    "            diabetes_percentage = (row['Diabetes'] / total) * 100  # Calculate percentage for Diabetes\n",
    "            axe[i].text(j, row['No Diabetes'] + row['Diabetes'] / 2, \n",
    "                        f\"{diabetes_percentage:.1f}%\", color=\"red\", ha=\"center\", va=\"top\")\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to fit the title\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normal_probability_plot(data, col):\n",
    "    \"\"\"\n",
    "    Generates a normal probability plot for the given data.\n",
    "    \n",
    "    Parameters:\n",
    "        data (array-like): The data for which to generate the plot.\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Generate a probability plot\n",
    "    stats.probplot(data, dist=\"norm\", plot=plt)\n",
    "    plt.title(\"Normal Probability Plot for \"+ col)\n",
    "    plt.xlabel(\"Theoretical Quantiles\")\n",
    "    plt.ylabel(\"Sample Quantiles\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in X.columns:\n",
    "#     normal_probability_plot(X[col], col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation matrix\n",
    "corr = X.corr()\n",
    "plt.figure(figsize=(30, 15))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Look for multicollinearity between the features and remove the features that are unnecessary\n",
    "# we already know the age so age_group is not needed\n",
    "columnsToDrop = ['Age_Group', 'MentHlth', 'HvyAlcoholConsump', 'NoDocbcCost', 'Smoker', 'Fruits', 'Mental_Health_Risk', 'Education', 'Income', 'Heart_Disease_Risk']\n",
    "X = X.drop(columns=columnsToDrop)\n",
    "X = feature_encoding(X)\n",
    "\n",
    "X_testing = X_testing.drop(columns=columnsToDrop)\n",
    "X_testing = feature_encoding(X_testing)\n",
    "\n",
    "X_testing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split validation set and train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import *\n",
    "is_equal_classes = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic splits of the data\n",
    "X_train, X_validation, y_train, y_validation =  data_splits(X, y) #   split data\n",
    "is_equal_classes = False\n",
    "X_train_scaled, X_validation_scaled, X_testing_Scaled = normalize_features(X_train, X_validation, X_testing) #   normalize data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Equal classes splits of the data\n",
    "# X_train, X_validation, y_train, y_validation =  data_splits_equal_classes(X, y) #   split data into equal classes. Meaning there is as many diabetes as non diabetes. But we remove some data\n",
    "# is_equal_classes = True\n",
    "# X_train_scaled, X_validation_scaled, X_testing_Scaled = normalize_features(X_train, X_validation, X_testing) #   normalize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import gen_batches\n",
    "def predict_in_batches(cls, X, batch_size=100):\n",
    "    \"\"\"\n",
    "    Make predictions for the input data in batches.\n",
    "    \n",
    "    Parameters:\n",
    "        cls (object): The trained classifier model.\n",
    "        X (array-like): The input data to make predictions for.\n",
    "        batch_size (int): The size of each batch.\n",
    "        \n",
    "    Returns:\n",
    "        array-like: The predictions for the input data.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    # Generate batches and make predictions for each batch\n",
    "    for batch in gen_batches(len(X), batch_size):\n",
    "        batch_predictions = cls.predict(X[batch])\n",
    "        predictions.append(batch_predictions)\n",
    "    \n",
    "    # Concatenate all batch predictions into a single array\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "def perform_grid_search(model, X_train_scaled, y_train, params):\n",
    "    print(\"Performing grid search for \", model)\n",
    "    # Define the cross-validation strategy\n",
    "    strat_kfold = StratifiedKFold(n_splits=10) # TODO\n",
    "\n",
    "    # Grid search for the model \n",
    "    #\"f1_macro\": Calculates F1-score per class and takes the average, treating all classes equally.\n",
    "    #\"f1_weighted\": Calculates F1-score per class and takes a weighted average, considering class imbalance.\n",
    "    grid_search = GridSearchCV(model, params, scoring='f1', cv=strat_kfold, n_jobs=10) # n_jobs=10 uses 10 parallel processes. Speeds up the process \n",
    "\n",
    "\n",
    "    if isinstance(model, RandomForestClassifier) or isinstance(model, SVC):\n",
    "        grid_search.fit(X_train_scaled, y_train.values.ravel())\n",
    "    else:\n",
    "        grid_search.fit(X_train, y_train)\n",
    "    best_param = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_ \n",
    "    print(\"Best parameters are:\", best_param)\n",
    "    print(\"Best score is:\", best_score)\n",
    "\n",
    "    # Return the fitted grid search objects\n",
    "    # return grid_search, best_param, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(model, X_train_scaled, y_train, X_validation_scaled, y_validation):\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_validation_pred = model.predict(X_validation_scaled)\n",
    "    validation_f1 = f1_score(y_validation, y_validation_pred)\n",
    "    print(\"Validation f1 score:\", validation_f1)\n",
    "    print(classification_report(y_validation, y_validation_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_summary(y_train, y_train_pred, y_validation, y_validation_pred):\n",
    "    train_f1 = f1_score(y_train, y_train_pred) \n",
    "    validation_f1 = f1_score(y_validation, y_validation_pred) \n",
    "    print(\"Train f1 score:\", train_f1)\n",
    "\n",
    "    print(\"Validation f1 score:\", validation_f1)\n",
    "    print(\"\\nTraining Set Classification Report:\")\n",
    "\n",
    "    print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "    print(\"Training Set Confusion Matrix:\")\n",
    "\n",
    "    print(confusion_matrix(y_train, y_train_pred))\n",
    "\n",
    "    print(\"\\nValidation Set Classification Report:\")\n",
    "\n",
    "    print(classification_report(y_validation, y_validation_pred))\n",
    "\n",
    "    print(\"Validation Set Confusion Matrix:\")\n",
    "\n",
    "    print(confusion_matrix(y_validation, y_validation_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_summary_one( y_validation, y_validation_pred):\n",
    "    validation_f1 = f1_score(y_validation, y_validation_pred) \n",
    "    print(\"Validation f1 score:\", validation_f1)\n",
    "    print(\"\\nValidation Set Classification Report:\")\n",
    "\n",
    "    print(classification_report(y_validation, y_validation_pred))\n",
    "\n",
    "    print(\"Validation Set Confusion Matrix:\")\n",
    "\n",
    "    print(confusion_matrix(y_validation, y_validation_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaggle_submission(model, X_testing_scaled, x_testing_id):\n",
    "    y_pred = model.predict(X_testing_scaled)\n",
    "    y_pred = pd.DataFrame({\n",
    "        'index': x_testing_id , \n",
    "        'Diabetes_binary': y_pred,\n",
    "    })\n",
    "\n",
    "    # Save the predictions to a CSV file\n",
    "    y_pred.to_csv(f'./{model}_y_pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission_pred(model, X_testing_scaled):\n",
    "    return model.predict(X_testing_scaled)\n",
    "\n",
    "\n",
    "def submission_proba(model, X_testing_scaled):\n",
    "    return model.predict_proba(X_testing_scaled)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### TESTING ####################\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "cls_randomforest = RandomForestClassifier(class_weight='balanced', \n",
    "                                          random_state=42, \n",
    "                                          max_depth=20, \n",
    "                                          max_leaf_nodes=200, \n",
    "                                          min_samples_leaf=20, n_estimators=500)\n",
    "\n",
    "param_grid_random_forest = {\n",
    "    'n_estimators': [10, 200, 300, 400, 500, 1000, 5000],\n",
    "    'max_depth': [20, 30],\n",
    "    'bootstrap': [True, False],\n",
    "}\n",
    "perform_grid_search(cls_randomforest, X_train_scaled, y_train, params= param_grid_random_forest)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_equal_classes:\n",
    "    print('Equal classes')\n",
    "    cls_randomforest = RandomForestClassifier(class_weight='balanced',\n",
    "                                          random_state=42, \n",
    "                                          max_depth=20, \n",
    "                                          max_leaf_nodes=200, \n",
    "                                           n_estimators=1000) #min_samples_leaf=20\n",
    "else:\n",
    "    cls_randomforest = RandomForestClassifier( class_weight={0: 1, 1: 3}, # class_weight='balanced',\n",
    "                                            random_state=42, \n",
    "                                            max_depth=20, \n",
    "                                            max_leaf_nodes=200, \n",
    "                                            n_estimators=1000) #min_samples_leaf=20\n",
    "\n",
    "model_training(cls_randomforest, X_train_scaled, y_train, X_validation_scaled, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = predict_in_batches(cls_randomforest, X_train_scaled)\n",
    "y_validation_pred = predict_in_batches(cls_randomforest, X_validation_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_summary(y_train, y_train_pred, y_validation, y_validation_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=12)#n_components=2\n",
    "pca.fit(X_train_scaled)\n",
    "X_train_pca = pca.transform(X_train_scaled)\n",
    "cls_randomforest_pca = RandomForestClassifier(class_weight='balanced', random_state=42, max_depth=20, max_leaf_nodes=200, n_estimators=1000)\n",
    "model_training(cls_randomforest_pca, X_train_pca, y_train, pca.transform(X_validation_scaled), y_validation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "cls_decision_tree = DecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "\n",
    "param_grid_decision_tree = {\n",
    "    'criterion': ['gini','entropy'],\n",
    "    'max_depth': [10, 20, 50],\n",
    "    'min_samples_leaf': [2, 3, 10],\n",
    "    'max_leaf_nodes': [5, 10, 50]\n",
    "}\n",
    "\n",
    "perform_grid_search(cls_decision_tree, X_train_scaled, y_train, params=param_grid_decision_tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_decision_tree = DecisionTreeClassifier(criterion='gini', class_weight={0: 1, 1: 3}, max_depth=10, min_samples_leaf=2, max_leaf_nodes=10)\n",
    "model_training(cls_decision_tree, X_train_scaled, y_train, X_validation_scaled, y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree with Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "cls_decision_tree = DecisionTreeClassifier(class_weight={0: 1, 1: 3}, criterion='gini', max_depth=10, min_samples_leaf=2, max_leaf_nodes=10)\n",
    "bagging_model = BaggingClassifier(estimator=cls_decision_tree, n_estimators=50, random_state=42)\n",
    "model_training(bagging_model, X_train_scaled, y_train, X_validation_scaled, y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree with Adaboosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Set up a decision stump as the weak learner\n",
    "base_estimator = DecisionTreeClassifier(max_depth=1, class_weight={0: 1, 1: 3}, criterion='gini')  # Decision stump class_weight='balanced'\n",
    "\n",
    "# Create an AdaBoost classifier using the decision tree as the base estimator\n",
    "ada_boost = AdaBoostClassifier(estimator=base_estimator, n_estimators=50, learning_rate=1.0, random_state=42)\n",
    "\n",
    "# Assuming `model_training` is a function that trains and evaluates the model\n",
    "model_training(ada_boost, X_train_scaled, y_train, X_validation_scaled, y_validation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_logistic = LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000, dual=False, C=5)\n",
    "param_grid_logistic = {\n",
    "    'max_iter':[1000, 2000, 3000],\n",
    "    'tol': [1e-3, 1e-4, 1e-5],\n",
    "    'C': [0.5, 1, 10],\n",
    "}\n",
    "perform_grid_search(cls_logistic, X_train_scaled, y_train, param_grid_logistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use best parameters to train the model\n",
    "if is_equal_classes:\n",
    "    print('Equal classes')\n",
    "    cls_logistic = LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000, dual=False, C=1, tol=1e-3)\n",
    "else:\n",
    "    cls_logistic = LogisticRegression(random_state=42, class_weight={0: 1, 1: 4}, max_iter=1000, dual=False, C=1, tol=1e-3)\n",
    "model_training(cls_logistic, X_train_scaled, y_train, X_validation_scaled, y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with Adaboosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logistic regression as the base estimator\n",
    "cls_logistic_regiression = LogisticRegression(random_state=42, class_weight={0: 1, 1: 4}, max_iter=1000, dual=False, C=1, tol=1e-3)\n",
    "\n",
    "# Create an AdaBoost classifier using logistic regression as the base estimator\n",
    "ada_boost = AdaBoostClassifier(estimator=cls_logistic_regiression, n_estimators=50, learning_rate=1.0, random_state=42)\n",
    "\n",
    "model_training(ada_boost, X_train_scaled, y_train, X_validation_scaled, y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the XGBoost classifier\n",
    "# booster [default= gbtree ]\n",
    "# Which booster to use. Can be gbtree, gblinear or dart; gbtree and dart use tree based models while gblinear uses linear functions.\n",
    "xgb_model = XGBClassifier(random_state=42, objective='binary:logistic', eval_metric='logloss') # binary:hinge\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'eval_metric': ['error', 'logloss', 'auc'],\n",
    "    'booster': ['gbtree', 'gblinear'],\n",
    "    'eval_metric': ['error'],\n",
    "    'n_estimators': [50, 100, 200, 5000],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "perform_grid_search(xgb_model, X_train_scaled, y_train, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier(random_state=42, max_depth=7,  objective='binary:logistic', eval_metric='error', booster='gbtree', n_estimators=5000)\n",
    "model_training(xgb_model, X_train_scaled, y_train, X_validation_scaled, y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_svm = SVC(class_weight='balanced', random_state=42, kernel='rbf', C=1)\n",
    "param_grid_svm = {\n",
    "    'kernel':['rbf', 'sigmoid'],\n",
    "    'shrinking': [True],\n",
    "    'C': [1, 10],\n",
    "}\n",
    "perform_grid_search(cls_svm, X_train_scaled, y_train, param_grid_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_svm = SVC(class_weight='balanced', random_state=42, kernel='rbf', C=1, shrinking=True)\n",
    "model_training(cls_svm, X_train_scaled, y_train, X_validation_scaled, y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "param_grid_svm = {\n",
    "    'weights':['distance', 'uniform', None],\n",
    "    'leaf_size': [10, 30, 50],\n",
    "}\n",
    "cls_knn = KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto')\n",
    "perform_grid_search(cls_knn, X_train_scaled, y_train, param_grid_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_knn = KNeighborsClassifier(n_neighbors=5,  weights='distance', algorithm='auto', leaf_size= 10)\n",
    "model_training(cls_knn, X_train_scaled, y_train, X_validation_scaled, y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the testing or validation data\n",
    "pred_array = []\n",
    "for model in [cls_randomforest, cls_logistic, bagging_model, ada_boost]:#xgb_model\n",
    "    pred_array.append(submission_pred(model, X_testing_Scaled))\n",
    "\n",
    "# Take the majority vote\n",
    "pred_array = np.array(pred_array)\n",
    "for i in range(len(pred_array[0])):\n",
    "    if sum([pred[i] for pred in pred_array]) >= pred_array.shape[0] / 2:\n",
    "        pred_array[0][i] = 1\n",
    "    else:\n",
    "        pred_array[0][i] = 0\n",
    "\n",
    "# classification_summary_one(y_validation, pred_array[0])\n",
    "# Save the predictions to a CSV file\n",
    "y_pred = pd.DataFrame({\n",
    "    'index': x_testing_id , \n",
    "    'Diabetes_binary': pred_array[0],\n",
    "})\n",
    "y_pred.to_csv('./y_pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_array = []\n",
    "for model in [cls_randomforest, cls_logistic, xgb_model, bagging_model, ada_boost]:\n",
    "    prob_array.append(submission_proba(model, X_validation_scaled))\n",
    "\n",
    "# Take most confident prediction\n",
    "prob_array = np.array(prob_array)\n",
    "for i in range(len(prob_array[0])):\n",
    "    if sum([prob[i] for prob in prob_array]) >= 2.3: # best is either 1.3 or 1.4 considering 3 models\n",
    "        prob_array[0][i] = 1\n",
    "    else:\n",
    "        prob_array[0][i] = 0\n",
    "pred_array = prob_array[0].astype(int)\n",
    "classification_summary_one(y_validation, pred_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_array = []\n",
    "for model in [cls_randomforest, cls_logistic, xgb_model, bagging_model, ada_boost]:\n",
    "    pred_array.append(submission_pred(model, X_validation_scaled))\n",
    "\n",
    "prob_array = []\n",
    "for model in [cls_randomforest, cls_logistic, xgb_model, bagging_model, ada_boost]:\n",
    "    prob_array.append(submission_proba(model, X_validation_scaled))\n",
    "\n",
    "# Take the majority vote\n",
    "pred_array = np.array(pred_array)\n",
    "for i in range(len(pred_array[0])):\n",
    "    if sum([pred[i] for pred in pred_array]) >= pred_array.shape[0] / 2 or sum([prob[i] for prob in prob_array]) >= 3.0: # logistic has been train on a equal class dataset. This is an experiment\n",
    "        pred_array[0][i] = 1\n",
    "    else:\n",
    "        pred_array[0][i] = 0\n",
    "\n",
    "classification_summary_one(y_validation, pred_array[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging + PCA models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_array = []\n",
    "\n",
    "for model in [cls_randomforest, cls_logistic, bagging_model, ada_boost]:#xgb_model\n",
    "    pred_array.append(submission_pred(model, X_validation_scaled))\n",
    "\n",
    "for pca_model in [cls_randomforest_pca]:\n",
    "    pred_array.append(submission_proba(pca_model, pca.transform(X_validation_scaled)))\n",
    "\n",
    "# Take the majority vote\n",
    "pred_array = np.array(pred_array)\n",
    "for i in range(len(pred_array[0])):\n",
    "    if sum([pred[i] for pred in pred_array]) >= pred_array.shape[0] / 2:\n",
    "        pred_array[0][i] = 1\n",
    "    else:\n",
    "        pred_array[0][i] = 0\n",
    "\n",
    "classification_summary_one(y_validation, pred_array[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
